(window.webpackJsonp=window.webpackJsonp||[]).push([[217],{536:function(t,a,s){"use strict";s.r(a);var n=s(17),r=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"数据归一化方法bn-ln-gn-in"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据归一化方法bn-ln-gn-in"}},[t._v("#")]),t._v(" 数据归一化方法BN/LN/GN/IN")]),t._v(" "),a("h2",{attrs:{id:"_0-introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_0-introduction"}},[t._v("#")]),t._v(" 0. Introduction")]),t._v(" "),a("p",[t._v("在神经网络的训练过程中，网络的收敛情况非常依赖于参数的初始化情况，使用"),a("code",[t._v("Normalization")]),t._v("的方法可以增强模型训练过程中的鲁棒性。目前常用的"),a("code",[t._v("Normalization")]),t._v("方法有"),a("strong",[t._v("Batch Normalization")]),t._v("、"),a("strong",[t._v("Layer Normalization")]),t._v("、"),a("strong",[t._v("Group Normalization")]),t._v("、"),a("strong",[t._v("Instance Normalization")]),t._v("四种方法，具体分别是指在一个"),a("code",[t._v("batch")]),t._v("的数据上分别在不同维度上做"),a("code",[t._v("Normalization")]),t._v("。如下图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n1norm.jpg",alt:""}})]),t._v(" "),a("p",[t._v("图中"),a("code",[t._v("N")]),t._v("表示一个"),a("code",[t._v("Batch")]),t._v("的大小，"),a("code",[t._v("WH")]),t._v("表示特征图宽高方向"),a("code",[t._v("resize")]),t._v("到一起后的维度方向，"),a("code",[t._v("C")]),t._v("表示不同的特征通道，"),a("code",[t._v("G")]),t._v("表示在通道方向做"),a("code",[t._v("Group Normalization")]),t._v("时每组包含的通道数的大小。")]),t._v(" "),a("h2",{attrs:{id:"_1-batch-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-batch-normalization"}},[t._v("#")]),t._v(" 1.Batch Normalization")]),t._v(" "),a("p",[a("code",[t._v("Batch Normalization")]),t._v("是谷歌的"),a("code",[t._v("Sergey Ioffe")]),t._v("等于2015年03月份提交的论文"),a("a",{attrs:{href:"https://arxiv.org/abs/1502.03167",target:"_blank",rel:"noopener noreferrer"}},[a("code",[t._v("Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift")]),a("OutboundLink")],1),t._v("中提出的。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n2bn.jpg",alt:""}})]),t._v(" "),a("p",[t._v("其中"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("是维度为"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"C"}})],1)],1)],1),t._v("的数据，分别求每个维度在"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"b"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"t"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"c"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("方向的均值和方差，然后进行归一化。值得注意的是方程")],1),t._v(" "),a("mjx-container",{staticClass:"MathJax",staticStyle:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"},attrs:{jax:"SVG",display:"true"}},[a("svg",{staticStyle:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.489ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"12.676ex",height:"2.321ex",role:"img",focusable:"false",viewBox:"0 -810 5602.9 1026","aria-hidden":"true"}},[a("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"}},[a("g",{attrs:{"data-mml-node":"math"}},[a("g",{attrs:{"data-mml-node":"msub"}},[a("g",{attrs:{"data-mml-node":"mi"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D466",d:"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"}})]),a("g",{attrs:{"data-mml-node":"mi",transform:"translate(523,-150) scale(0.707)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])]),a("g",{attrs:{"data-mml-node":"mo",transform:"translate(1094.7,0)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"2190",d:"M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"}})]),a("g",{attrs:{"data-mml-node":"mi",transform:"translate(2372.5,0)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D6FE",d:"M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"}})]),a("g",{attrs:{"data-mml-node":"TeXAtom","data-mjx-texclass":"ORD",transform:"translate(2915.5,0)"}},[a("g",{attrs:{"data-mml-node":"mover"}},[a("g",{attrs:{"data-mml-node":"msub"}},[a("g",{attrs:{"data-mml-node":"mi"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D465",d:"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"}})]),a("g",{attrs:{"data-mml-node":"mi",transform:"translate(605,-150) scale(0.707)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])]),a("g",{attrs:{"data-mml-node":"mo",transform:"translate(449.5,16) translate(-250 0)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"5E",d:"M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"}})])])]),a("g",{attrs:{"data-mml-node":"mo",transform:"translate(4036.7,0)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"2B",d:"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"}})]),a("g",{attrs:{"data-mml-node":"mi",transform:"translate(5036.9,0)"}},[a("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D6FD",d:"M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"}})])])])]),a("mjx-assistive-mml",{staticStyle:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"},attrs:{unselectable:"on",display:"block"}},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("msub",[a("mi",[t._v("y")]),a("mi",[t._v("i")])],1),a("mo",{attrs:{stretchy:"false"}},[t._v("←")]),a("mi",[t._v("γ")]),a("mrow",{attrs:{"data-mjx-texclass":"ORD"}},[a("mover",[a("msub",[a("mi",[t._v("x")]),a("mi",[t._v("i")])],1),a("mo",{attrs:{stretchy:"false"}},[t._v("^")])],1)],1),a("mo",[t._v("+")]),a("mi",[t._v("β")])],1)],1)],1),a("p",[t._v("相当于对归一化后的数据做了线性变换，这里"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B3"}})],1)],1)],1),t._v("和"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B2"}})],1)],1)],1),t._v("都是在网络训练过程中需要学习的参数。根据上述"),a("code",[t._v("BN")]),t._v("的计算方式可求得反向传播的链路图：")],1),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n3BNcircuit.jpg",alt:""}})]),t._v(" "),a("p",[t._v("由此使用"),a("code",[t._v("Batch Normalization Layer")]),t._v("时，其对应的反向和前向推理代码为，参考自"),a("a",{attrs:{href:"https://cs231n.github.io/assignments2022/assignment2/",target:"_blank",rel:"noopener noreferrer"}},[t._v("CS231N homework2"),a("OutboundLink")],1),t._v("：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## Forward")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("batchnorm_forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Forward pass for batch normalization.\n\n    During training the sample mean and (uncorrected) sample variance are\n    computed from minibatch statistics and used to normalize the incoming data.\n    During training we also keep an exponentially decaying running mean of the\n    mean and variance of each feature, and these averages are used to normalize\n    data at test-time.\n\n    At each timestep we update the running averages for mean and variance using\n    an exponential decay based on the momentum parameter:\n\n    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n    running_var = momentum * running_var + (1 - momentum) * sample_var\n\n    Note that the batch normalization paper suggests a different test-time\n    behavior: they compute sample mean and variance for each feature using a\n    large number of training images rather than using a running average. For\n    this implementation we have chosen to use running averages instead since\n    they do not require an additional estimation step; the torch7\n    implementation of batch normalization also uses running averages.\n\n    Input:\n    - x: Data of shape (N, D)\n    - gamma: Scale parameter of shape (D,)\n    - beta: Shift paremeter of shape (D,)\n    - bn_param: Dictionary with the following keys:\n      - mode: \'train\' or \'test\'; required\n      - eps: Constant for numeric stability\n      - momentum: Constant for running mean / variance.\n      - running_mean: Array of shape (D,) giving running mean of features\n      - running_var Array of shape (D,) giving running variance of features\n\n    Returns a tuple of:\n    - out: of shape (N, D)\n    - cache: A tuple of values needed in the backward pass\n    """')]),t._v("\n    mode "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mode"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    eps "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"eps"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    momentum "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"momentum"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    running_mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"running_mean"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dtype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    running_var "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"running_var"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dtype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" mode "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n        avg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        var "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        std "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x_hat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" avg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("std "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" eps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_hat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" gamma "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" beta\n        \n        shape "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"shape"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"axis"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" avg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" std"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_hat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n          running_mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" momentum "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" momentum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" avg\n          running_var "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_var "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" momentum "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" momentum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" var\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" mode "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n        x_hat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" running_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("running_var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" eps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_hat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" gamma "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" beta\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" ValueError"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Invalid forward batchnorm mode \"%s\"'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" mode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Store the updated running means back into bn_param")]),t._v("\n    bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"running_mean"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_mean\n    bn_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"running_var"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_var\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## Backward")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("batchnorm_backward_alt")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Alternative backward pass for batch normalization.\n\n    For this implementation you should work out the derivatives for the batch\n    normalizaton backward pass on paper and simplify as much as possible. You\n    should be able to derive a simple expression for the backward pass.\n    See the jupyter notebook for more hints.\n\n    Note: This implementation should expect to receive the same cache variable\n    as batchnorm_backward, but might not use all of the values in the cache.\n\n    Inputs / outputs: Same as batchnorm_backward\n    """')]),t._v("\n    dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dgamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dbeta "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    _"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" std"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_hat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cache "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# expand cache")]),t._v("\n    S "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("                     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# helper function")]),t._v("\n    \n    dbeta "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" order"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'F'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# derivative w.r.t. beta")]),t._v("\n    dgamma "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" x_hat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" order"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'F'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# derivative w.r.t. gamma")]),t._v("\n    \n    dx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" gamma "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" std"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# temporarily initialize scale value")]),t._v("\n    dx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("dx  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" S"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("x_hat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("x_hat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" S"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# derivative w.r.t. unnormalized x")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dgamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dbeta\n\n")])])]),a("p",[t._v("在以上代码中，"),a("code",[t._v("BatchNorm")]),t._v("层在训练结束推理时使用的是训练时得到的"),a("code",[t._v("running average")]),t._v("和"),a("code",[t._v("running variance")]),t._v("，在反向传播梯度时是根据链式法则求出"),a("code",[t._v("BN")]),t._v("层整体的梯度公式来计算梯度，可以减少中间变量的存储和计算，减少运算量和内存占用。")]),t._v(" "),a("h2",{attrs:{id:"_3-layer-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-layer-normalization"}},[t._v("#")]),t._v(" 3.Layer Normalization")]),t._v(" "),a("p",[a("strong",[t._v("Batch Normalization")]),t._v("在使用过程中依赖batch size的大小，当模型比较复杂，占用内存过多时很难使用大的"),a("code",[t._v("batch size")]),t._v("进行网络训练，这时BN的效果会受到限制，"),a("code",[t._v("2016")]),t._v("年"),a("code",[t._v("Hinton")]),t._v("等提出的"),a("a",{attrs:{href:"https://arxiv.org/pdf/1607.06450.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("LayerNormalization"),a("OutboundLink")],1),t._v("克服了这些问题，可以作为batch size 较小时"),a("code",[t._v("Batch Normalization")]),t._v("的一种替代方案。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n4ln.jpg",alt:""}})]),t._v(" "),a("p",[t._v("其中，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"H"}})],1)],1)],1),t._v("表示当前层隐层单元的数量，当使用的是卷积神经网络时，"),a("code",[t._v("Layer Normalization")]),t._v("是作用在卷积核作用在输入上得到的输出的每个通道上，输出的每个通道算做一层，在该层上做"),a("code",[t._v("Normalization")]),t._v("。")],1),t._v(" "),a("p",[t._v("代码实现：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("layernorm_forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ln_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Forward pass for layer normalization.\n    During both training and test-time, the incoming data is normalized per data-point,\n    before being scaled by gamma and beta parameters identical to that of batch normalization.\n    Note that in contrast to batch normalization, the behavior during train and test-time for\n    layer normalization are identical, and we do not need to keep track of running averages\n    of any sort.\n    Input:\n    - x: Data of shape (N, D)\n    - gamma: Scale parameter of shape (D,)\n    - beta: Shift paremeter of shape (D,)\n    - ln_param: Dictionary with the following keys:\n        - eps: Constant for numeric stability\n    Returns a tuple of:\n    - out: of shape (N, D)\n    - cache: A tuple of values needed in the backward pass\n    """')]),t._v("\n    out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    eps "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ln_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"eps"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ln_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setdefault"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mode'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("       "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# same as batchnorm in train mode")]),t._v("\n    ln_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setdefault"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'axis'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# over which axis to sum for grad")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("atleast_2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# assure 2D to perform transpose")]),t._v("\n\n    out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batchnorm_forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ln_param"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# same as batchnorm")]),t._v("\n    out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T                                                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# transpose back")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("layernorm_backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Backward pass for layer normalization.\n    For this implementation, you can heavily rely on the work you\'ve done already\n    for batch normalization.\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, D)\n    - cache: Variable of intermediates from layernorm_forward.\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs x, of shape (N, D)\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n    """')]),t._v("\n    dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dgamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dbeta "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dgamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dbeta "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batchnorm_backward_alt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# same as batchnorm backprop")]),t._v("\n    dx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# transpose back dx")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dgamma"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dbeta\n")])])]),a("p",[t._v("从上面代码可以看到，"),a("code",[t._v("Layer Normalization")]),t._v("是在每个样本的每层输出上实现的，因此可以复用"),a("code",[t._v("Batch Normalization")]),t._v("的实现。")]),t._v(" "),a("h2",{attrs:{id:"_4-group-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-group-normalization"}},[t._v("#")]),t._v(" 4.Group Normalization")]),t._v(" "),a("p",[a("code",[t._v("Group Normalization")]),t._v("是2018年06月份HeKaiMing等提出的"),a("a",{attrs:{href:"https://arxiv.org/abs/1803.08494",target:"_blank",rel:"noopener noreferrer"}},[t._v("论文"),a("OutboundLink")],1),t._v("中发表的方法，作为"),a("code",[t._v("Batch Normalization")]),t._v("的另一种替代。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n5gn.jpg",alt:""}})]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## pytorch example")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\nx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GroupNorm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noutput "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# equal to ")]),t._v("\ngx1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ngx2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nmu1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gx1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmu2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gx2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstd1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gx1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstd2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("var"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gx2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mu1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("std1  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mu2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("std2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"_6-instance-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-instance-normalization"}},[t._v("#")]),t._v(" 6.Instance Normalization")]),t._v(" "),a("p",[t._v("Instance Normalization 是2017年1月份"),a("code",[t._v("Dmitry Ulyanov")]),t._v("等发表的论文"),a("a",{attrs:{href:"https://arxiv.org/abs/1701.02096",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis"),a("OutboundLink")],1),t._v("中的提出的方法，其作用在单个样本的一个通道上，相当于"),a("code",[t._v("num_groups=1")]),t._v("的"),a("code",[t._v("Group Normalization")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://day-pic-1311699660.cos.ap-nanjing.myqcloud.com/image/n6InstanceNorm.jpg",alt:""}})]),t._v(" "),a("InArticleAdsense",{attrs:{"data-ad-client":"ca-pub-8685746128991385","data-ad-slot":"2974191661"}}),t._v(" "),a("h3",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("a",{attrs:{href:"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("1.https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/mantasu/cs231n/blob/master/assignment2/cs231n/layers.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("2.https://github.com/mantasu/cs231n/blob/master/assignment2/cs231n/layers.py"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://wandb.ai/wandb_fc/GroupNorm/reports/Group-Normalization-in-Pytorch-With-Examples---VmlldzoxMzU0MzMy",target:"_blank",rel:"noopener noreferrer"}},[t._v("3.Group Normalization in Pytorch (With Examples)"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("4.GROUPNORM"),a("OutboundLink")],1)])])])],1)}),[],!1,null,null,null);a.default=r.exports}}]);